---
layout: post
title: Hadoop HA 高可用分布式集群搭建
description: ""
categories: [Linux]
keywords: [Linux，大数据,hadoop]
---

Hadoop 高可用 (High Availability) 分为 HDFS 高可用和 YARN 高可用，两者的实现基本类似，但 HDFS NameNode 对数据存储及其一致性的要求比 YARN ResourceManger 高得多，所以它的实现也更加复杂。

## 集群规划

我这里用的是 Centos Linux 7.7，通过虚拟机进行链接克隆，总共是三台虚拟机。集群的规划如下：

| 节点名称 | NN   | Journalnode | DN   | ZKFC | RM   | ZK   | NM   |
| -------- | ---- | ----------- | ---- | ---- | ---- | ---- | ---- |
| hadoop01 | ✔    | ✔           | ✔    | ✔    |      | ✔    | ✔    |
| hadoop02 | ✔    | ✔           | ✔    | ✔    | ✔    | ✔    | ✔    |
| Hadoop03 |      | ✔           | ✔    | ✔    | ✔    | ✔    | ✔    |

## 基本配置

首先肯定是跟之前的 hadoop 分布式搭建相同，第一步做的都是关闭防火墙，修改主机名，配置 hosts 主机映射，然后再配置 ssh 免密登陆。

- 将三台虚拟机的防火墙关闭

  ```shell
  systemctl stop firewalld
  ```

- 修改主机名

  使用如下命名修改主机名，完成之后`exit`登出虚拟机，然后重新登陆，修改完成。

  ```shell
  hostnamectl set-hostname hadoop01
  hostnamectl set-hostname hadoop02
  hostnamectl set-hostname hadoop03	
  ```

- 修改主机映射

  配置完成之后，最好用`ping`命令查看一下，检验是否配置成功。

  ```shell
  vi /etc/hosts
  192.168.56.119 hadoop01
  192.168.56.120 hadoop02
  192.168.56.121 hadoop03
  ```

- 配置 ssh 免密登陆

  首先到每一台的机子上使用第一个命令生成公钥/私钥对，然后再将本地生成的公钥拷贝到需要登陆的虚拟机上即可。其中第二个命令最好也给自己拷贝一份。配置完成之后可以使用`ssh hostname`检验是否成功。

  ```shell
  1.ssh-keygen -t rsa
  2.ssh-copy-id hostname
  ```

## 环境搭建

我将所需要用到的环境安装包统一放在`/opt/software`下，然后解压到`/usr/local/apps/`目录下。

[![lHwx4x.md.png](https://s2.ax1x.com/2020/01/13/lHwx4x.md.png)](https://imgchr.com/i/lHwx4x)

### 安装 JDK 环境

- 首先将 JDK 安装包解压，并修改文件夹名称

  ```shell
  tar -zxvf jdk.tar.gz -C /usr/local/apps/
  mv jdk1.8.0_131 jdk
  ```

- 配置环境变量

  ```shell
  vi /etc/profile
  export JAVA_HOME=/usr/local/apps/jdk
  export PATH=$JAVA_HOME/bin:$PATH
  ```

- 装载配置文件

  ```
  source /etc/profile
  ```

  然后使用 `java -version`查看 jdk 版本，jdk 配置完成。

  [![lH0jsg.md.png](https://s2.ax1x.com/2020/01/13/lH0jsg.md.png)](https://imgchr.com/i/lH0jsg)

- 将配置完成的 jdk 拷贝到另外两台虚拟机上

  ```shell
  [root@hadoop01 bin]# scp /etc/profile hadoop02:/etc/profile
  [root@hadoop01 bin]# scp /etc/profile hadoop03:/etc/profile
  [root@hadoop01 apps]# scp -r jdk/ hadoop02:$PWD
  [root@hadoop01 apps]# scp -r jdk/ hadoop03:$PWD
  [root@hadoop02 bin]# source /etc/profile
  [root@hadoop03 bin]# source /etc/profile
  ```

  记得在另外两台虚拟机上创建 apps 文件夹，到此 jdk 的安装就完成了。

### Zookeeper 集群搭建

首先安装 zookeeper 的环境， 搭建完成之后启动 zookeeper 集群。

#### 安装 zookeeper 集群

- 首先将 zookeeer 安装包解压，并修改文件夹名称

  ```shell
  tar -zxvf zookeeper-3.4.5.tar.gz -C /usr/local/apps/
  mv zookeeper-3.4.5 zookeeper
  ```

- 在 zookeeper 目录下创建 data 和 log 文件夹

  ```shell
  mkdir data
  mkdir log
  ```

- 进入 zookeeper 的 conf 目录下修改 zoo.cfg

  ```shell
  cp zoo_sample.cfg zoo.cfg
  vi zoo.cfg
  
  dataDir=/usr/local/apps/zookeeper/data
  dataLogDir=/usr/local/apps/zookeeper/log
  
  server.1=hadoop01:2888:3888
  server.2=hadoop02:2888:3888
  server.3=hadoop03:2888:3888
  ```

- 在 data 文件下创建 myid 文件

  ```shell
  [root@hadoop01 data]# echo 1 > myid
  ```

- 配置 zookeeper 的环境变量，并装载环境变量

  ```
  export ZOOKEEPER_HOME=/usr/local/apps/zookeeper
  export PATH=$JAVA_HOME/bin:$ZOOKEEPER_HOME:/bin:$PATH
  ```

- 将配置好的 zookeeper 发送到另外两台虚拟机上，并修改对应的 myid 文件

  ```shell
  [root@hadoop01 bin]# scp -r zookeeper/ hadoop02:$PWD
  [root@hadoop01 bin]# scp -r zookeeper/ hadoop03:$PWD
  [root@hadoop02 data]# echo 2 > myid
  [root@hadoop03 data]# echo 3 > myid
  [root@hadoop02 bin]# source /etc/profile
  [root@hadoop03 bin]# source /etc/profile
  ```

#### 启动 zookeeper 集群

- 节点启动

  ```shell
  [root@hadoop01 bin]# ./zkServer.sh start
  ```

- 查看节点状态

  ![lHsSdH.png](https://s2.ax1x.com/2020/01/13/lHsSdH.png)

### 安装 Hadoop 环境

hadoop 的搭建我分为两部分，一部分是环境配置，一部分是修改配置文件。

#### 环境搭建

- 首相将 hadoop 安装包解压，并修改文件夹名称

  ```shell
  [root@hadoop01 apps]# mv hadoop-2.6.0 hadoop
  ```

- 配置环境变量，并装载

  ```shell
  [root@hadoop01 apps]# vi /etc/profile
  export HADOOP_HOME=/usr/local/apps/hadoop
  export PATH=$JAVA_HOME/bin:$ZOOKEEPER_HOME:/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$PATH
  [root@hadoop01 apps]# source /etc/profile
  ```

#### 修改配置文件

- hadoop-env.sh

  ```shell
  export JAVA_HOME=/usr/local/apps/jdk
  ```

  

- core-site.xml

  ```shell
  <configuration>
          <property>
                  <name>fs.defaultFS</name>
                  <value>hdfs://mycluster</value>
          </property>
  
          <property>
                  <name>hadoop.tmp.dir</name>
                  <value>/usr/local/apps/hadoop/data/tmp</value>
          </property>
  
          <property>
                  <name>ha.zookeeper.quorum</name>
                  <value>hadoop01:2181,hadoop02:2181,hadoop03:2181</value>
          </property>
  </configuration>
  ```

- hdfs-site.xml
  
  ```shell
  <configuration>
  	<property>
  		<name>dfs.replication</name>
  		<value>2</value>
  	</property>
  
  	<property>
  		<name>dfs.nameservices</name>
  		<value>mycluster</value>
  	</property>
  	
  	<property>
  		<name>dfs.namenode.name.dir</name>
  		<value>/usr/local/apps/hadoop/dfs/namenode</value>
  	</property>
  
  	<property>
  		<name>dfs.datanode.data.dir</name>
  		<value>/usr/local/apps/hadoop/dfs/datanode</value>
  	</property>
  
  	<property>
  		<name>dfs.ha.namenodes.mycluster</name>
  		<value>nn1,nn2</value>
  	</property>
  	
  	<property>
  		<name>dfs.namenode.rpc-address.mycluster.nn1</name>
  		<value>hadoop01:8020</value>
  	</property>
  	
  	<property>
  		<name>dfs.namenode.rpc-address.mycluster.nn2</name>
  		<value>hadoop02:8020</value>
  	</property>
  
  	<property>
  		<name>dfs.namenode.http-address.mycluster.nn1</name>
  		<value>hadoop01:50070</value>
  	</property>
  
  	<property>
  		<name>dfs.namenode.http-address.mycluster.nn2</name>
  		<value>hadoop02:50070</value>
  	</property>
  
  	<property>
  		<name>dfs.namenode.shared.edits.dir</name>
  		<value>qjournal://hadoop01:8485;hadoop02:8485;hadoop03:8485/mycluster</value>
  	</property>
  
  	<property>
  		<name>dfs.journalnode.edits.dir</name>
  		<value>/usr/local/apps/hadoop/data/journalnode</value>
  	</property>
  
  	<property>
  		<name>dfs.ha.fencing.methods</name>
  		<value>
  			sshfence
  			shell(/bin/true)
  		</value>
  	</property>
    
          <property>
                  <name>dfs.client.failover.proxy.provider.mycluster</name
                  <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
          </property>
    
  	<property>
  		<name>dfs.ha.automatic-failover.enabled</name>
  		<value>true</value>
  	</property>
  
  </configuration>
  ```
  
- Mapped-site.xml

  ```
  <configuration>
          <property>
                  <name>mapreduce.framework.name</name>
                  <value>yarn</value>
          </property>
  </configuration>
  ```

- yarn-site.xml

  ```shell
  <configuration>
  
  <!-- Site specific YARN configuration properties -->
  	<property>
  		<name>yarn.nodemanager.aux-services</name>
  		<value>mapreduce_shuffle</value>
  	</property>		
  
  	<property>
  		<name>yarn.resourcemanager.ha.enabled</name>
  		<value>true</value>
  	</property>
  
  	<property>
  		<name>yarn.resourcemanager.cluster-id</name>
  		<value>rmCluster</value>
  	</property>
  
  	<property>
  		<name>yarn.resourcemanager.ha.rm-ids</name>
  		<value>rm1,rm2</value>
  	</property>
  
  	<property>
  		<name>yarn.resourcemanager.hostname.rm1</name>
  		<value>hadoop02</value>
  	</property>
  
  	<property>
  		<name>yarn.resourcemanager.hostname.rm2</name>
  		<value>hadoop03</value>
  	</property>
  
  	<property>
  		<name>yarn.resourcemanager.webapp.address.rm1</name>
  		<value>hadoop02:8088</value>
  	</property>	
  	
  	<property>
  		<name>yarn.resourcemanager.webapp.address.rm2</name>
  		<value>hadoop03:8088</value>
  	</property>
  
  	<property>
  		<name>yarn.resourcemanager.zk-address</name>
  		<value>hadoop01:2181,hadoop02:2181,hadoop03:2181</value>
  	</property>
  	
  	<property>
  		<name>yarn.resourcemanager.recovery.enabled</name>
  		<value>true</value>
  	</property>
  
  	<property>
  		<name>yarn.resourcemanager.store.class</name>
  		<value>org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore</value>
  	</property>
  	
  </configuration>
  ```

- slaves

  ```shell
  hadoop01
  hadoop02
  hadoop03	
  ```

- 配置完成之后将 hadoop 文件夹和环境变量分发到另外两台虚拟机

  ```shell
  [root@hadoop01 apps]# scp -r hadoop/ hadoop02:$PWD
  [root@hadoop01 apps]# scp -r hadoop/ hadoop03:$PWD
  [root@hadoop01 hadoop]# scp /etc/profile hadoop02:/etc/profile
  [root@hadoop01 hadoop]# scp /etc/profile hadoop03:/etc/profile
  ```

#### 启动集群

- 再配置的各个 journalnode 节点启动该进程

  ```shell
  [root@hadoop01 hadoop]# hadoop-daemon.sh start journalnode
  [root@hadoop02 hadoop]# hadoop-daemon.sh start journalnode
  [root@hadoop03 hadoop]# hadoop-daemon.sh start journalnode
  
  ```
  ![lH2OSA.png](https://s2.ax1x.com/2020/01/13/lH2OSA.png)

- 在第一次 namenode 节点上格式化文件系统

  ```shell
  [root@hadoop01 hadoop]# hadoop namenode -format
  ```

  ![lHR1p9.png](https://s2.ax1x.com/2020/01/13/lHR1p9.png)

- 格式化 ZKFC 

  ```shell
  [root@hadoop01 sbin]# hdfs zkfc -formatZK
  ```

- 启动 hadoop

  ```shell
  [root@hadoop01 sbin]# start-dfs.sh
  ```

- 同步两个 namenode 的元数据

  在 hadoop02 下执行一下命令：

  ```shell
  [root@hadoop01 sbin]# hadoop namenode -bootstrapStandby
  ```

到此 hdfs 的高可用已经搭建好了，可以去浏览器查看 ui 界面。

![lHWmjI.png](https://s2.ax1x.com/2020/01/13/lHWmjI.png)

![lHWMHf.png](https://s2.ax1x.com/2020/01/13/lHWMHf.png)

- 启动 yarn

  这里还需要另外在 hadoop03 上手动启动 yarn

  ```shell
  [root@hadoop02 bin]# start-yarn.sh
  [root@hadoop03 apps]# yarn-daemon.sh start resourcemanager
  ```

- 启动 mapreduce 任务历史服务器

  ```shell
  [root@hadoop01 apps]# mr-jobhistory-daemon.sh start historyserver
  [root@hadoop02 apps]# mr-jobhistory-daemon.sh start historyserver
  [root@hadoop03 apps]# mr-jobhistory-daemon.sh start historyserver
  ```

  ![lHWT8H.png](https://s2.ax1x.com/2020/01/13/lHWT8H.png)

![lHWqKI.png](https://s2.ax1x.com/2020/01/13/lHWqKI.png)

- 检验高可用

  只需要`kill -9 `杀死对应的进程即可，standby 节点会自动成为 active 节点。

  
